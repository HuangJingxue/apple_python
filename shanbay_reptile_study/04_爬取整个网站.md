# 爬取整个网站

#突破反爬虫拿到数据#

- header变身
- 使用代理

```python
import requests
import random
from bs4 import BeautifulSoup

def get_douban_books(url, proxies):
  headers = {
    'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/76.0.3809.132 Safari/537.36'
  }
  # 使用代理爬取数据
  res = requests.get(url, proxies=proxies, headers=headers)
  soup = BeautifulSoup(res.text, 'html.parser')
  items = soup.find_all('div', class_='pl2')
  for i in items:
    tag = i.find('a')
    name = tag['title']
    link = tag['href']
    print(name, link)

url = 'https://book.douban.com/top250?start={}'
urls = [url.format(num * 25) for num in range(10)]
# IP 代理池（瞎写的并没有用）
proxies_list = [
  {
    "http": "http://10.10.1.10:3128",
    "https": "http://10.10.1.10:1080",
  },
  {
    "http": "http://10.10.1.11:3128",
    "https": "http://10.10.1.11:1080",
  },
  {
    "http": "http://10.10.1.12:3128",
    "https": "http://10.10.1.12:1080",
  }
]
for i in urls:
  # 从 IP 代理池中随机选择一个
  proxies = random.choice(proxies_list)
  get_douban_books(i, proxies)
```

## 君子协议

先打开该网站的 **robots.txt** 查看哪些页面可以爬取数据，哪些页面禁止爬取数据

爬虫开发者的道德自持和企业经营者的良知才是避免触碰法律底线的根本所在。

